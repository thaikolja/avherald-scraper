diff --git a/.env b/.env
new file mode 100644
index 0000000..7803162
--- /dev/null
+++ b/.env
@@ -0,0 +1,5 @@
+# The URL, default "https://avherald.com", or use proxied URL
+BASE_URL="https://avherald.com"
+
+# The path to the SQLite database file
+DATABASE_FILE_PATH="./output/data.sqlite"
\ No newline at end of file
diff --git a/.env.example b/.env.example
new file mode 100644
index 0000000..7803162
--- /dev/null
+++ b/.env.example
@@ -0,0 +1,5 @@
+# The URL, default "https://avherald.com", or use proxied URL
+BASE_URL="https://avherald.com"
+
+# The path to the SQLite database file
+DATABASE_FILE_PATH="./output/data.sqlite"
\ No newline at end of file
diff --git a/.gitignore b/.gitignore
index cd40fe7..31f923d 100644
--- a/.gitignore
+++ b/.gitignore
@@ -1,6 +1,7 @@
-/output
+/output/*
+!/output/.gitkeep
 *.sqlite
-.idea/**/*
+.idea
 
 # User-specific stuff
 .idea/**/workspace.xml
@@ -230,7 +231,6 @@ celerybeat.pid
 *.sage.py
 
 # Environments
-.env
 .venv
 env/
 venv/
diff --git a/.gitlab-ci.yml b/.gitlab-ci.yml
index ecba078..e9351c4 100644
--- a/.gitlab-ci.yml
+++ b/.gitlab-ci.yml
@@ -6,10 +6,10 @@ image: python:3.13-slim
 before_script:
   - pip install --upgrade pip
   - pip install .
+  - pip install -r requirements.txt
   - pip install pytest
 
 test:
   stage: test
   script:
     - pytest tests/
-
diff --git a/CHANGELOG.md b/CHANGELOG.md
new file mode 100644
index 0000000..8f57481
--- /dev/null
+++ b/CHANGELOG.md
@@ -0,0 +1,9 @@
+# Changelog
+
+## v1.0.2
+
+* Changed request URL to go through a proxy to bypasss IP block
+
+## v1.0.1
+
+* Changed the location of the output to `/output/data.sqlite`
diff --git a/avherald_scraper/avherald_scraper.py b/avherald_scraper/avherald_scraper.py
index 80d69d8..04ea38f 100644
--- a/avherald_scraper/avherald_scraper.py
+++ b/avherald_scraper/avherald_scraper.py
@@ -1,26 +1,31 @@
 # -*- coding: utf-8 -*-
+"""
+This script scrapes incident data from avherald.com.
 
-# Copyright (C) 2025 by Kolja Nolte
-# kolja.nolte@gmail.com
-# https://www.kolja-nolte.com
-#
-# This script scrapes incident data from avherald.com.
-# Please read the README.md for more information.
-#
-# This work is licensed under the MIT License. You are free to use, share,
-# and adapt this work, provided that you Include the original copyright notice.
-#
-# For more information, see the LICENSE file.
-#
-# Author:    Kolja Nolte
-# Email:     kolja.nolte@gmail.com
-# License:   MIT License
-# Date:      2025
-# Package:   avherald-scraper
+It includes functions for scraping, parsing, and storing incident data in a SQLite database.
+
+Copyright (C) 2025 by Kolja Nolte
+kolja.nolte@gmail.com
+https://www.kolja-nolte.com
+
+This work is licensed under the MIT License. You are free to use, share,
+and adapt this work, provided that you Include the original copyright notice.
+
+For more information, see the LICENSE file.
+
+Author:    Kolja Nolte
+Email:     kolja.nolte@gmail.com
+License:   MIT License
+Date:      2025
+Package:   avherald-scraper
+"""
 
 # Import the requests library for making HTTP requests.
 import requests
 
+# Import the dotenv library for loading environment variables.
+import dotenv
+
 # Import the BeautifulSoup library for parsing HTML.
 from bs4 import BeautifulSoup
 
@@ -45,33 +50,58 @@ import sqlite3
 # Import calendar module for UTC timestamp.
 import calendar
 
+env_path = dotenv.find_dotenv('.env', False)
+
+# if not os.path.exists(env_path):
+# 	raise FileNotFoundError(
+# 		f"Could not find .env file."
+# 		f"Please create one in the root directory based on the .env.example file."
+# 	)
+
+# Load environment variables from a .env file.
+dotenv.load_dotenv(env_path)
+
+# List of required keys from the .env file
+required_keys = [
+	"BASE_URL",
+	"DATABASE_FILE_PATH"
+]
+
+# Check that all required keys are set
+missing_keys = [key for key in required_keys if not os.getenv(key)]
+if missing_keys:
+	raise EnvironmentError(f"Missing required environment variables: {', '.join(missing_keys)}")
+
 # Define the base URL for avherald.com.
-BASE_URL = "https://avherald.com/"
-# Define the headers to be sent with the request, mimicking a browser.
-HEADERS = {
-	'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'
-}
+BASE_URL = os.getenv("BASE_URL")
 
-# Check if the output directory exists.
-if not os.path.isdir('./output'):
-	# Create the output directory if it doesn't exist, allowing intermediate directories to be created.
-	os.makedirs('./output', exist_ok=True)
+# Define the path to the SQLite database file.
+DATABASE_FILE_PATH = os.getenv("DATABASE_FILE_PATH")
 
 # Define the regular expression string for matching dates.
 DATE_REGEX_STR = r"(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d{1,2}(?:st|nd|rd|th)\s+\d{4}"
+
 # Compile the date regular expression.
 DATE_REGEX = re.compile(DATE_REGEX_STR)
+
 # Compile the regular expression for removing ordinal suffixes.
 ORDINAL_SUFFIX_REGEX = re.compile(r"(?<=\d)(st|nd|rd|th)")
 
+# Create the output directory from the database file path.
+output_directory = os.path.dirname(DATABASE_FILE_PATH)
+
+# Check if the output directory exists.
+if not os.path.isdir(output_directory):
+	# Create the output directory if it doesn't exist, allowing intermediate directories to be created.
+	os.makedirs(output_directory, exist_ok=True)
+
 
-# /**
-#  * Converts a date string (e.g. 'Mar 31st 2025') into a UNIX timestamp.
-#  *
-#  * @param date_string The date string to convert.
-#  * @param show_details Whether to print details if parsing fails.
-#  * @return The UNIX timestamp or None if parsing fails.
-#  */
+#
+# Converts a date string (e.g. 'Mar 31st 2025') into a UNIX timestamp.
+#
+# @param date_string The date string to convert.
+# @param show_details Whether to print details if parsing fails.
+# @return The UNIX timestamp or None if parsing fails.
 def date_to_timestamp(date_string, show_details=False):
 	# Check if the date string is empty.
 	if not date_string:
@@ -95,13 +125,12 @@ def date_to_timestamp(date_string, show_details=False):
 		return None
 
 
-# /**
-#  * Processes the original title string.
-#  *
-#  * @param original_title The original title string to process.
-#  * @param show_details Whether to print details if parsing fails.
-#  * @return A dict with keys: 'cleaned_title', 'cause', 'date_string', 'location'
-#  */
+#
+# Processes the original title string.
+#
+# @param original_title The original title string to process.
+# @param show_details Whether to print details if parsing fails.
+# @return A dict with keys: 'cleaned_title', 'cause', 'date_string', 'location'
 def process_title(original_title, show_details=False):
 	# Strip leading/trailing whitespace from the title.
 	title = original_title.strip()
@@ -164,26 +193,34 @@ def process_title(original_title, show_details=False):
 	return result
 
 
-# /**
-#  * Scrapes a single page of avherald.com incidents.
-#  *
-#  * @param page_url The URL of the page to scrape.
-#  * @param show_details Whether to print details during scraping.
-#  * @return A tuple: (list_of_incidents_on_page, next_page_url or None)
-#  */
+#
+# Scrapes a single page of avherald.com incidents.
+#
+# @param page_url The URL of the page to scrape.
+# @param show_details Whether to print details during scraping.
+# @return A tuple: (list_of_incidents_on_page, next_page_url or None)
 def scrape_single_page(page_url, show_details=False):
 	# Check if details should be shown.
 	if show_details:
 		# Print the URL being scraped.
 		print(f"Attempting to scrape: {page_url}")
+
 	# Try to fetch the page content.
 	try:
+		# Define the headers to be sent with the request, mimicking a browser.
+		headers = {
+			'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'
+		}
+
 		# Make a GET request to the page URL with specified headers and timeout.
-		response = requests.get(page_url, headers=HEADERS, timeout=20)
+		response = requests.get(page_url, headers=headers, timeout=20)
+
 		# Raise an exception for bad status codes.
 		response.raise_for_status()
+
 		# Set the encoding to utf-8.
 		response.encoding = 'utf-8'
+
 	# Catch a Timeout exception.
 	except requests.exceptions.Timeout:
 		# Check if details should be shown.
@@ -305,11 +342,10 @@ def scrape_single_page(page_url, show_details=False):
 	return page_incidents, next_page_url
 
 
-# /**
-#  * Creates the 'incidents' table with the appropriate columns if it doesn't exist.
-#  *
-#  * @param conn The database connection object.
-#  */
+#
+# Creates the 'incidents' table with the appropriate columns if it doesn't exist.
+#
+# @param conn The database connection object.
 def create_table_if_not_exists(conn):
 	# Define the SQL query to create the 'incidents' table if it doesn't exist.
 	sql = """
@@ -328,18 +364,18 @@ def create_table_if_not_exists(conn):
 	conn.commit()
 
 
-# /**
-#  * Inserts an incident into the database.
-#  *
-#  * @param conn The database connection object.
-#  * @param incident The incident dictionary to insert.
-#  * @return True if inserted, False if skipped (duplicate or news category).
-#  */
+#
+# Insert an incident into the database.
+#
+# @param conn The database connection object.
+# @param incident The incident data to insert.
+# @return True if a row was inserted, False otherwise.
 def insert_incident(conn, incident):
 	# Skip incidents with the category "news"
 	if incident['category'].lower() == "news":
+		# Returns False if the category is new.
 		return False
-		
+
 	# Define the SQL query to insert an incident into the database, ignoring duplicates.
 	sql = """
     INSERT OR IGNORE INTO incidents (category, title, location, cause, timestamp, url)
@@ -362,13 +398,12 @@ def insert_incident(conn, incident):
 	return cur.rowcount == 1  # True if inserted, False if skipped
 
 
-# /**
-#  * Inserts a list of incidents into the database.
-#  *
-#  * @param conn The database connection object.
-#  * @param incidents A list of incident dictionaries to insert.
-#  * @return A tuple: (inserted_count, skipped_count)
-#  */
+#
+# Inserts a list of incidents into the database.
+#
+# @param conn The database connection object.
+# @param incidents A list of incident dictionaries to insert.
+# @return A tuple: (inserted_count, skipped_count)
 def insert_incidents(conn, incidents):
 	# Initialize the inserted count.
 	inserted = 0
@@ -388,22 +423,16 @@ def insert_incidents(conn, incidents):
 	return inserted, skipped
 
 
-# /**
-#  * Scrapes avherald.com for incident data and stores it in a database.
-#  *
-#  * @param max_pages_to_scrape The maximum number of pages to scrape.
-#  * @param request_delay_seconds The delay in seconds between requests.
-#  * @param database_file The path to the SQLite database file.
-#  * @param show_details Whether to print details during scraping.
-#  */
-def scrape(
-	max_pages_to_scrape=3,
-	request_delay_seconds=1,
-	database_file='../output/data.sqlite',
-	show_details=False
-):
+#
+# Scrapes avherald.com for incident data and stores it in a database.
+#
+# @param max_pages_to_scrape The maximum number of pages to scrape.
+# @param request_delay_seconds The delay in seconds between requests.
+# @param database_file The path to the SQLite database file.
+# @param show_details Whether to print details during scraping.
+def scrape(max_pages_to_scrape=3, request_delay_seconds=3, show_details=True):
 	# Connect to the SQLite database.
-	conn = sqlite3.connect(database_file)
+	conn = sqlite3.connect(DATABASE_FILE_PATH)
 	# Create the 'incidents' table if it doesn't exist.
 	create_table_if_not_exists(conn)
 	# Set the initial URL to the base URL.
@@ -453,10 +482,4 @@ def scrape(
 		# Print a message indicating that scraping is finished.
 		print(f"\n--- Finished Scraping ---")
 		# Print the total number of pages scraped.
-		print(f"Scraped a total of {pages_scraped} pages and stored new incidents into {database_file}.")
-
-
-# Check if the script is being run as the main program.
-if __name__ == "__main__":
-	# Call the scrape function with specified parameters.
-	scrape(max_pages_to_scrape=1, request_delay_seconds=1, show_details=True)
+		print(f"Scraped a total of {pages_scraped} pages and stored new incidents into {DATABASE_FILE_PATH}.")
diff --git a/main.py b/main.py
index 8e58e54..cf38427 100644
--- a/main.py
+++ b/main.py
@@ -30,22 +30,6 @@ file for storing the scraped data, and whether to display detailed output during
 # Import the avherald_scraper module
 from avherald_scraper import avherald_scraper
 
-#
-# Define the maximum number of pages to scrape
-MAX_PAGES_TO_SCRAPE = 1
-
-#
-# Define the delay in seconds between requests
-REQUEST_DELAY_SECONDS = 3
-
-#
-# Define the path to the database file
-DATABASE_FILE = './output/data.sqlite'
-
-#
-# Define whether to show detailed output during scraping
-SHOW_DETAILS = True  # Set to False to suppress detailed output
-
 
 def main():
 	"""
@@ -54,20 +38,18 @@ def main():
 	This function orchestrates the scraping process by calling the scrape function
 	from the avherald_scraper module with specified configuration parameters.
 	"""
+
 	# Call the scrape function with specified parameters
 	avherald_scraper.scrape(
 		# Specify the maximum number of pages to scrape
-		max_pages_to_scrape=MAX_PAGES_TO_SCRAPE,
+		max_pages_to_scrape=1,
 		# Specify the delay in seconds between requests
-		request_delay_seconds=REQUEST_DELAY_SECONDS,
-		# Specify the path to the database file
-		database_file=DATABASE_FILE,
+		request_delay_seconds=3,
 		# Specify whether to show detailed output
-		show_details=SHOW_DETAILS
+		show_details=True
 	)
 
 
-#
 # Check if the script is being run as the main module
 if __name__ == "__main__":
 	# Call the main function to start the scraping process
diff --git a/output/.gitkeep b/output/.gitkeep
new file mode 100644
index 0000000..e69de29
diff --git a/requirements.txt b/requirements.txt
index 4313b1b..e6dcd9a 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -1,14 +1,19 @@
 beautifulsoup4==4.13.3
 certifi==2025.1.31
 charset-normalizer==3.4.1
+flake8==7.2.0
 idna==3.10
 iniconfig==2.1.0
 lxml==5.3.1
+mccabe==0.7.0
 packaging==24.2
 pluggy==1.5.0
+pycodestyle==2.13.0
+pyflakes==3.3.2
 pytest==8.3.5
 python-dotenv==1.1.0
 requests==2.32.3
+setuptools==79.0.1
 soupsieve==2.6
 typing_extensions==4.13.1
 urllib3==2.3.0
diff --git a/setup.py b/setup.py
index 1ff8802..40d5a4e 100644
--- a/setup.py
+++ b/setup.py
@@ -4,14 +4,21 @@ import pathlib
 here = pathlib.Path(__file__).parent.resolve()
 
 # load long description from README.md if exists
-long_description = ""
+long_description = (
+	'The Aviation Herald Scraper is a Python tool designed to automatically extract basic incident information from the front page headlines of avherald.com and '
+	'stores them in a local SQLite database.'
+)
+
+# load long description from README.md if exists
 readme = here / "README.md"
+
+# load long description from README.md if exists
 if readme.exists():
 	long_description = readme.read_text(encoding="utf-8")
 
 setup(
 	name="avherald-scraper",
-	version="1.0.1",
+	version="1.0.2",
 	description="Scrapes aviation incident data from AV Herald website",
 	long_description=long_description,
 	long_description_content_type="text/markdown",
